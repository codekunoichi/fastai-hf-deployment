{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"fastai \u2192 Hugging Face Deployment Playbook","text":""},{"location":"#purpose","title":"Purpose","text":"<p>This repository documents the end-to-end journey of deploying a fastai model from a local notebook environment to a live, publicly accessible application on Hugging Face Spaces using Gradio.</p> <p>The focus here is not on model accuracy or architecture tuning, but on everything that happens after training: - exporting a model - handling environment and version mismatches - managing binaries and Git workflows - adapting to library and API drift - getting a real deployment running</p> <p>This repo exists to capture the practical realities of ML deployment that are often skipped in tutorials.</p>"},{"location":"#background","title":"Background","text":"<p>The work here builds on concepts from the fastai course:</p> <ul> <li>Chapter 1: Training a simple image classifier (e.g., \u201cIs it a bird?\u201d)</li> <li>Chapter 2: Extending the same workflow to custom datasets</li> </ul> <p>Using those foundations, the classifier was extended to detect: - Grizzly bears - Black bears - Teddy bears</p> <p>The trained model was then exported and deployed as a live application.</p>"},{"location":"#live-demo","title":"Live Demo","text":"<p>You can try the deployed application here:</p> <p>\ud83d\udc49 Bear Detector \u2013 Hugging Face Space https://huggingface.co/spaces/rgiri2025/bear-detector</p>"},{"location":"#key-learnings-captured-in-this-repo","title":"Key Learnings Captured in This Repo","text":"<p>This repository documents lessons learned the hard way, including:</p> <ul> <li>Using Git LFS to manage large binary artifacts (<code>model.pkl</code>, example images)</li> <li>Generating and using Hugging Face Personal Access Tokens (PATs) for Git authentication</li> <li>Why Python version mismatches (3.10 vs 3.12) can break fastai model loading</li> <li>The importance of pinning dependencies (e.g. <code>numpy&lt;2</code>)</li> <li>Why blindly running <code>pip -U</code> in notebooks is risky</li> <li>How Gradio APIs changed over time and why older tutorials break</li> <li>How to read Hugging Face Build logs vs Container logs</li> <li>What a minimal, working <code>app.py</code> for fastai + Gradio looks like today</li> </ul>"},{"location":"#about-the-tutorials","title":"About the Tutorials","text":"<p>An older tutorial was used as an initial reference:</p> <ul> <li>Tanishq\u2019s Gradio + Hugging Face blog (2021):   https://www.tanishq.ai/blog/posts/2021-11-16-gradio-huggingface.html</li> </ul> <p>That blog is helpful conceptually, but much of the code no longer works due to: - library version changes - deprecated Gradio APIs - updated Hugging Face deployment requirements</p> <p>This repository documents the necessary updates and fixes to make the workflow work in a modern (2025) environment.</p>"},{"location":"#repository-structure","title":"Repository Structure","text":"<ul> <li><code>apps/</code> \u2013 deployable application examples (e.g. bear-detector)</li> <li><code>docs/</code> \u2013 detailed journey logs, troubleshooting, and checklists</li> <li>Hugging Face Spaces \u2013 hosts the live demo and model artifacts</li> </ul>"},{"location":"#why-this-exists","title":"Why This Exists","text":"<p>This is a learning artifact, not just a demo.</p> <p>The goal is to leave behind a clear, honest record of:</p> <p>\u201cWhat actually breaks when you try to deploy a real ML model \u2014 and how to fix it.\u201d</p> <p>If you are moving from notebooks to real deployments, this repo is for you.</p>"},{"location":"bear-detector-journey/","title":"Bear Detector Deployment Journey (My Brain Tattoo \ud83e\udde0)","text":"<p>This document captures the real end-to-end journey of taking a trained fastai model and deploying it as a live application on Hugging Face Spaces using Gradio.</p> <p>This is intentionally written as a battle log \u2014 not a polished tutorial \u2014 so future\u2011me (and others) can remember what actually broke, why it broke, and how it was fixed.</p>"},{"location":"bear-detector-journey/#1-goal","title":"1. Goal","text":"<p>The goal of this repository is to document the post\u2013model\u2011training journey:</p> <ul> <li>What it takes to move from a trained fastai model (<code>learn.export</code>)</li> <li>To a deployed, publicly accessible application</li> <li>With a stable API/UI for inference</li> <li>Using Hugging Face Spaces + Gradio</li> </ul> <p>This work is part of a broader effort to deeply understand practical deep learning, not just model training.</p> <p>For example: - You can train a bird detector or bear classifier in a notebook - But how do you actually deploy it so someone else can use it? - What breaks when you leave the notebook world?</p> <p>This repo answers those questions.</p>"},{"location":"bear-detector-journey/#2-what-worked-locally","title":"2. What Worked Locally","text":"<ul> <li>The model was trained locally on macOS using Python 3.12</li> <li>fastai training and inference worked as expected</li> <li><code>learn.export(\"model.pkl\")</code> completed successfully</li> <li>A local Gradio app (<code>app.py</code>) ran without issues on <code>localhost:7860</code></li> </ul> <p>At this stage, everything appeared correct \u2014 which is often misleading.</p>"},{"location":"bear-detector-journey/#3-what-broke-on-kaggle-numpy-2x-pip-u","title":"3. What Broke on Kaggle (NumPy 2.x + <code>pip -U</code>)","text":"<p>The first failure happened when the notebook was imported into Kaggle.</p>"},{"location":"bear-detector-journey/#what-went-wrong","title":"What went wrong","text":"<ul> <li>The first notebook cell used <code>pip -U</code></li> <li>This upgraded NumPy to 2.x</li> <li>fastai (and compiled dependencies) were built against NumPy 1.x</li> <li>Result: cryptic runtime errors</li> </ul>"},{"location":"bear-detector-journey/#key-realization","title":"Key realization","text":"<p>Blind upgrades (<code>pip -U</code>) are dangerous in ML environments.</p>"},{"location":"bear-detector-journey/#fix","title":"Fix","text":"<p>Pin versions explicitly:</p> <pre><code>!pip -q install \"numpy&lt;2\" \"fastai==2.8.5\" \"ddgs\"\n</code></pre> <p>Once versions were pinned, the notebook worked again.</p>"},{"location":"bear-detector-journey/#4-what-broke-on-hugging-face-python-310-vs-312-pickle","title":"4. What Broke on Hugging Face (Python 3.10 vs 3.12 Pickle)","text":"<p>The next major failure happened during deployment to Hugging Face Spaces.</p>"},{"location":"bear-detector-journey/#what-went-wrong_1","title":"What went wrong","text":"<ul> <li>Hugging Face Spaces default to Python 3.10</li> <li>The model was exported using Python 3.12</li> <li><code>model.pkl</code> is a pickle that embeds Python code objects</li> <li>Resulting error:   <code>TypeError: code expected at most 16 arguments, got 18</code></li> </ul>"},{"location":"bear-detector-journey/#root-cause","title":"Root cause","text":"<p>Pickled models are not portable across Python minor versions.</p>"},{"location":"bear-detector-journey/#fix_1","title":"Fix","text":"<p>Explicitly set the Python version in the Space <code>README.md</code>:</p> <pre><code>python_version: 3.12\n</code></pre> <p>After this change, the model loaded correctly.</p>"},{"location":"bear-detector-journey/#5-what-broke-in-gradio-api-drift","title":"5. What Broke in Gradio (API Drift)","text":"<p>The tutorial being followed used deprecated Gradio APIs.</p>"},{"location":"bear-detector-journey/#what-broke","title":"What broke","text":"<ul> <li><code>gr.inputs.Image</code></li> <li><code>gr.outputs.Label</code></li> </ul> <p>These APIs no longer exist in modern Gradio.</p>"},{"location":"bear-detector-journey/#fix_2","title":"Fix","text":"<p>Update to the new API:</p> <pre><code>gr.Image(type=\"filepath\")\ngr.Label(num_top_classes=3)\n</code></pre> <p>This is a reminder that deployment tutorials age quickly.</p>"},{"location":"bear-detector-journey/#6-git-gymnastics-on-hugging-face-auth-large-files","title":"6. Git Gymnastics on Hugging Face (Auth + Large Files)","text":"<p>Several Git-related issues surfaced during deployment.</p>"},{"location":"bear-detector-journey/#authentication","title":"Authentication","text":"<ul> <li>Hugging Face no longer allows password-based Git auth</li> <li>A Personal Access Token (PAT) is required</li> </ul>"},{"location":"bear-detector-journey/#binary-files","title":"Binary files","text":"<ul> <li><code>.pkl</code> and <code>.jpg</code> files are considered binaries</li> <li>Direct pushes are rejected</li> </ul>"},{"location":"bear-detector-journey/#fix_3","title":"Fix","text":"<p>Use Git LFS:</p> <pre><code>git lfs install\ngit lfs track \"*.pkl\"\ngit lfs track \"*.jpg\"\ngit add .gitattributes\ngit commit -m \"Track model and assets with git-lfs\"\n</code></pre> <p>This creates lightweight pointers instead of pushing raw binaries.</p>"},{"location":"bear-detector-journey/#7-final-working-configuration-key-snippets","title":"7. Final Working Configuration (Key Snippets)","text":""},{"location":"bear-detector-journey/#space-readmemd","title":"Space <code>README.md</code>","text":"<pre><code>sdk: gradio\npython_version: 3.12\n</code></pre>"},{"location":"bear-detector-journey/#model-loading-cpu-safe","title":"Model loading (CPU-safe)","text":"<pre><code>learn = load_learner(\"model_resnet18.pkl\", cpu=True)\n</code></pre>"},{"location":"bear-detector-journey/#gradio-launch","title":"Gradio launch","text":"<pre><code>gr.Interface(...).launch()\n</code></pre> <p>(No <code>share=True</code> needed on Hugging Face.)</p>"},{"location":"bear-detector-journey/#8-repeatable-deployment-checklist","title":"8. Repeatable Deployment Checklist","text":"<ul> <li>[ ] Pin NumPy (<code>numpy&lt;2</code>)</li> <li>[ ] Pin fastai and Gradio versions</li> <li>[ ] Avoid <code>pip -U</code> in notebooks</li> <li>[ ] Export model using the same Python version as deployment</li> <li>[ ] Set <code>python_version</code> explicitly in HF Space README</li> <li>[ ] Update Gradio APIs to current syntax</li> <li>[ ] Use PAT for Hugging Face Git auth</li> <li>[ ] Track binaries with Git LFS</li> <li>[ ] Check Build logs before Container logs</li> <li>[ ] Add startup prints when debugging deployments</li> </ul>"},{"location":"bear-detector-journey/#final-reflection","title":"Final Reflection","text":"<p>This journey reinforced a key lesson:</p> <p>\u201cWorks locally\u201d does not mean \u201cready for deployment.\u201d</p> <p>Deployment is where assumptions surface, and where real engineering begins.</p>"},{"location":"bear-detector-journey/#who-this-is-for","title":"Who This Is For","text":"<ul> <li>fastai learners moving beyond notebooks</li> <li>anyone deploying pickled ML models</li> <li>engineers new to Hugging Face Spaces</li> </ul>"},{"location":"bear-detector-journey/#what-i-learned-the-real-takeaways","title":"What I Learned (The Real Takeaways)","text":"<ol> <li> <p>Model training is the easy part    Getting a model to train and infer in a notebook is only the beginning. The real work starts once you try to move that model outside the notebook.</p> </li> <li> <p>Environment drift is the #1 deployment killer    Small differences in Python versions, NumPy versions, or library APIs can completely break a deployment, even when the code looks identical.</p> </li> <li> <p>Pickled models are not portable artifacts    A <code>.pkl</code> file is not a neutral format. It encodes Python internals and must be created and loaded under compatible Python versions.</p> </li> <li> <p>Blind upgrades (<code>pip -U</code>) are dangerous in ML    Upgrading dependencies without understanding transitive impacts can silently break compiled libraries and invalidate an otherwise working environment.</p> </li> <li> <p>Tutorials rot faster than APIs    Deployment tutorials age quickly. Expect API drift and be prepared to read library docs and error messages instead of copying code blindly.</p> </li> <li> <p>Git is part of the ML stack    Authentication, large file handling, and repo hygiene are not \u201cside tasks.\u201d They are core skills required to ship ML systems.</p> </li> <li> <p>Binary artifacts need deliberate handling    Models and assets require tools like Git LFS or external storage. Treating them like normal source files will fail at scale.</p> </li> <li> <p>Logs tell a story \u2014 if you read the right ones    Build logs, container logs, and startup prints each answer different questions. Knowing where to look saves hours of guesswork.</p> </li> <li> <p>Deployment requires thinking in systems, not scripts    Successful deployment is about understanding how environments, tools, CI/CD, and runtime constraints interact \u2014 not just writing Python code.</p> </li> <li> <p>Documentation is part of engineering, not an afterthought     Writing down what broke, why it broke, and how it was fixed turns frustration into reusable knowledge \u2014 for future-me and others.</p> </li> </ol> <p>This document exists so I never have to relearn these lessons the hard way again.</p>"},{"location":"hf-spaces-checklist/","title":"HF Spaces Checklist","text":"<ul> <li>\u201cmodule \u2018gradio\u2019 has no attribute inputs\u201d \u2192 old tutorial \u2192 use gr.Image()</li> <li>\u201cpush rejected contains binary files\u201d \u2192 add LFS tracking or avoid committing binaries</li> <li>\u201ccode expected at most 16 arguments got 18\u201d \u2192 python mismatch \u2192 set python_version: 3.12 in HF README</li> <li>\u201cNumPy 1.x compiled module cannot run in NumPy 2.x\u201d \u2192 pin numpy&lt;2, avoid pip -U</li> </ul>"},{"location":"troubleshooting/","title":"Troubleshooting Guide (fastai \u2192 Hugging Face Deployment)","text":"<p>This document captures common failure modes encountered when moving from a local fastai notebook to a deployed application on Hugging Face Spaces, along with their root causes and fixes.</p> <p>Each item follows a symptom \u2192 cause \u2192 fix pattern.</p>"},{"location":"troubleshooting/#1-space-stuck-on-building-or-pushing-image","title":"1. Space stuck on \u201cBuilding\u201d or \u201cPushing image\u201d","text":"<ul> <li>Cause: Large dependency layers (torch, gradio), Hugging Face infra lag, or cached Docker layers</li> <li>Fix: Be patient and refresh; slim down <code>requirements.txt</code>; push a small commit to retrigger the build; scan the Build logs for early <code>ERROR</code> lines</li> </ul>"},{"location":"troubleshooting/#2-build-logs-show-success-but-space-ui-still-says-building","title":"2. Build logs show success, but Space UI still says \u201cBuilding\u201d","text":"<ul> <li>Cause: UI lag while the container is transitioning to runtime</li> <li>Fix: Check Container logs for <code>===== Application Startup =====</code>; refresh the Space page</li> </ul>"},{"location":"troubleshooting/#3-error-typeerror-code-expected-at-most-16-arguments-got-18","title":"3. Error: <code>TypeError: code expected at most 16 arguments, got 18</code>","text":"<ul> <li>Cause: Python version mismatch (model exported on Python 3.12, Space running Python 3.10)</li> <li>Fix: Set <code>python_version: 3.12</code> explicitly in the Space <code>README.md</code>, or re-export the model using the deployment Python version</li> </ul>"},{"location":"troubleshooting/#4-error-attributeerror-module-gradio-has-no-attribute-inputs","title":"4. Error: <code>AttributeError: module 'gradio' has no attribute 'inputs'</code>","text":"<ul> <li>Cause: Outdated tutorial code using deprecated Gradio APIs (<code>gr.inputs.*</code>, <code>gr.outputs.*</code>)</li> <li>Fix: Update to modern Gradio components, e.g. <code>gr.Image()</code> and <code>gr.Label()</code></li> </ul>"},{"location":"troubleshooting/#5-crash-on-kagglecolab-module-compiled-using-numpy-1x-cannot-run-in-numpy-2x","title":"5. Crash on Kaggle/Colab: \u201cmodule compiled using NumPy 1.x cannot run in NumPy 2.x\u201d","text":"<ul> <li>Cause: NumPy 2.x installed while compiled dependencies expect NumPy 1.x</li> <li>Fix: Pin NumPy explicitly (<code>numpy&lt;2</code>), avoid <code>pip -U</code>, and restart the kernel after installation</li> </ul>"},{"location":"troubleshooting/#6-pip-warning-dependency-resolver-does-not-currently-take-into-account","title":"6. pip warning: \u201cdependency resolver does not currently take into account\u2026\u201d","text":"<ul> <li>Cause: Mixing environment-pinned packages with forced upgrades</li> <li>Fix: Remove <code>-U</code>; pin versions explicitly; install dependencies once at the top of the notebook; restart kernel</li> </ul>"},{"location":"troubleshooting/#7-git-push-to-hugging-face-fails-authentication","title":"7. Git push to Hugging Face fails authentication","text":"<ul> <li>Cause: Hugging Face no longer supports password-based Git authentication</li> <li>Fix: Use a Hugging Face Personal Access Token (PAT) as the Git password; store it via the system credential helper</li> </ul>"},{"location":"troubleshooting/#8-push-rejected-contains-binary-files-offending-files-modelpkl-jpg","title":"8. Push rejected: \u201ccontains binary files\u201d / \u201coffending files: model.pkl, .jpg\u201d","text":"<ul> <li>Cause: Large binary files pushed without Git LFS/Xet tracking</li> <li>Fix: Track binaries with Git LFS (<code>*.pkl</code>, <code>*.jpg</code>); commit <code>.gitattributes</code>; re-add files; migrate history if already committed</li> </ul>"},{"location":"troubleshooting/#9-filenotfounderror-when-calling-load_learner-locally","title":"9. <code>FileNotFoundError</code> when calling <code>load_learner()</code> locally","text":"<ul> <li>Cause: Incorrect working directory or undefined path variable</li> <li>Fix: Use relative paths (<code>load_learner(\"model.pkl\")</code>) or confirm working directory with <code>os.getcwd()</code> / <code>Path.cwd()</code></li> </ul>"},{"location":"troubleshooting/#10-incorrect-probability-display-prediction-correct-probability-shows-00000","title":"10. Incorrect probability display (prediction correct, probability shows 0.0000)","text":"<pre><code>- **Cause:** Incorrect indexing into `probs` assuming a fixed class order\n- **Fix:** Map probabilities to labels explicitly using `learn.dls.vocab` or index via `pred_idx`\n</code></pre> <p>If you encounter a new failure mode, add it here with the same symptom \u2192 cause \u2192 fix structure so this document continues to evolve as a practical deployment reference.</p>"}]}